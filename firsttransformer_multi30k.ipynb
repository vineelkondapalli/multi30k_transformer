{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPsQRC4wicsRnmyMajLoaRD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineelkondapalli/multi30k_transformer/blob/main/firsttransformer_multi30k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qyjS8rcEaPfR",
        "outputId": "1c09bc69-2ae7-4ae5-f12e-357adffff062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "# !pip install torch==2.3.0 torchtext==0.18.0 torchvision --upgrade\n",
        "# !pip install -U spacy\n",
        "# !pip install datasets\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# !python -m spacy download de_core_news_sm\n",
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 1. IMPORTS AND SETUP\n",
        "# =============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from datasets import load_dataset\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "import spacy\n",
        "import math\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 2. DATA PIPELINE\n",
        "# =============================================================================\n",
        "# Load tokenizers\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# Load the dataset\n",
        "multi30k_dataset = load_dataset(\"bentrevett/multi30k\")\n",
        "\n",
        "# Build vocabularies\n",
        "def yield_tokens(data_iter, language):\n",
        "    language_tokenizers = {'de': tokenize_de, 'en': tokenize_en}\n",
        "    for data_sample in data_iter:\n",
        "        yield language_tokenizers[language](data_sample[language])\n",
        "\n",
        "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
        "\n",
        "SRC_VOCAB = build_vocab_from_iterator(yield_tokens(multi30k_dataset['train'], 'de'), min_freq=1, specials=special_symbols, special_first=True)\n",
        "SRC_VOCAB.set_default_index(UNK_IDX)\n",
        "\n",
        "TRG_VOCAB = build_vocab_from_iterator(yield_tokens(multi30k_dataset['train'], 'en'), min_freq=1, specials=special_symbols, special_first=True)\n",
        "TRG_VOCAB.set_default_index(UNK_IDX)\n",
        "\n",
        "# Define DataLoader and collate_fn\n",
        "def text_transform(tokenizer, vocab, sos_idx, eos_idx):\n",
        "    def transform(text_sample):\n",
        "        tokens = tokenizer(text_sample.rstrip(\"\\n\"))\n",
        "        return torch.cat((torch.tensor([sos_idx]), torch.tensor(vocab(tokens)), torch.tensor([eos_idx])))\n",
        "    return transform\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = [], []\n",
        "    for sample in batch:\n",
        "        # The fix is to add SOS_IDX and EOS_IDX to the calls below\n",
        "        src_batch.append(text_transform(tokenize_de, SRC_VOCAB, SOS_IDX, EOS_IDX)(sample['de']))\n",
        "        trg_batch.append(text_transform(tokenize_en, TRG_VOCAB, SOS_IDX, EOS_IDX)(sample['en']))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    trg_batch = pad_sequence(trg_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "    return src_batch.to(device), trg_batch.to(device)\n",
        "\n",
        "# --- IMPORTANT: Re-run the DataLoader definitions after changing the function ---\n",
        "train_dataloader = DataLoader(multi30k_dataset['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "valid_dataloader = DataLoader(multi30k_dataset['validation'], batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(multi30k_dataset['test'], batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "print(\"Data Pipeline Ready.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 3. MODEL DEFINITION\n",
        "# =============================================================================\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        assert hid_dim % n_heads == 0\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        Q, K, V = self.fc_q(query), self.fc_k(key), self.fc_v(value)\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        if mask is not None: energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        x = self.fc_o(x)\n",
        "        return x, attention\n",
        "\n",
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.self_attention_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
        "        self.feedforward_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src, src_mask):\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        src = self.self_attention_layer_norm(src + self.dropout(_src))\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        src = self.feedforward_layer_norm(src + self.dropout(_src))\n",
        "        return src\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=200):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device) for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "    def forward(self, src, src_mask):\n",
        "        batch_size, src_len = src.shape\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.self_attention_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
        "        self.feedforward_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        trg = self.self_attention_layer_norm(trg + self.dropout(_trg))\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        trg = self.encoder_attention_layer_norm(trg + self.dropout(_trg))\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        trg = self.feedforward_layer_norm(trg + self.dropout(_trg))\n",
        "        return trg, attention\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=200):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device) for _ in range(n_layers)])\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        batch_size, trg_len = trg.shape\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        output = self.fc_out(trg)\n",
        "        return output, attention\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        return trg_mask\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        return output, attention\n",
        "\n",
        "print(\"All model classes defined.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 4. INSTANTIATION AND TRAINING SETUP\n",
        "# =============================================================================\n",
        "INPUT_DIM = len(SRC_VOCAB)\n",
        "OUTPUT_DIM = len(TRG_VOCAB)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.3\n",
        "DEC_DROPOUT = 0.3\n",
        "\n",
        "enc = Encoder(INPUT_DIM, HID_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
        "dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
        "\n",
        "net = Seq2Seq(enc, dec, PAD_IDX, PAD_IDX, device).to(device)\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "net.apply(initialize_weights)\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-3)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=.1)\n",
        "\n",
        "print(\"Model instantiated and ready for training.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 5. TRAINING LOOP\n",
        "# =============================================================================\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src, trg = batch\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, trg = batch\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(net, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(net, valid_dataloader, criterion)\n",
        "    end_time = time.time()\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(net.state_dict(), 'transformer-model.pt')\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {end_time - start_time:.0f}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "print(\"\\nFinished Training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "af5LHiP8ccFz",
        "outputId": "3a4abcf3-310f-4c0a-bb14-3c196341fb7f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Data Pipeline Ready.\n",
            "All model classes defined.\n",
            "Model instantiated and ready for training.\n",
            "\n",
            "Starting training...\n",
            "Epoch: 01 | Time: 9s\n",
            "\tTrain Loss: 5.332 | Train PPL: 206.892\n",
            "\t Val. Loss: 4.487 |  Val. PPL:  88.818\n",
            "Epoch: 02 | Time: 9s\n",
            "\tTrain Loss: 4.329 | Train PPL:  75.851\n",
            "\t Val. Loss: 4.107 |  Val. PPL:  60.750\n",
            "Epoch: 03 | Time: 9s\n",
            "\tTrain Loss: 3.965 | Train PPL:  52.728\n",
            "\t Val. Loss: 3.808 |  Val. PPL:  45.080\n",
            "Epoch: 04 | Time: 9s\n",
            "\tTrain Loss: 3.692 | Train PPL:  40.137\n",
            "\t Val. Loss: 3.613 |  Val. PPL:  37.072\n",
            "Epoch: 05 | Time: 9s\n",
            "\tTrain Loss: 3.504 | Train PPL:  33.250\n",
            "\t Val. Loss: 3.510 |  Val. PPL:  33.433\n",
            "Epoch: 06 | Time: 9s\n",
            "\tTrain Loss: 3.380 | Train PPL:  29.372\n",
            "\t Val. Loss: 3.460 |  Val. PPL:  31.808\n",
            "Epoch: 07 | Time: 9s\n",
            "\tTrain Loss: 3.292 | Train PPL:  26.888\n",
            "\t Val. Loss: 3.428 |  Val. PPL:  30.829\n",
            "Epoch: 08 | Time: 9s\n",
            "\tTrain Loss: 3.229 | Train PPL:  25.258\n",
            "\t Val. Loss: 3.394 |  Val. PPL:  29.773\n",
            "Epoch: 09 | Time: 9s\n",
            "\tTrain Loss: 3.180 | Train PPL:  24.048\n",
            "\t Val. Loss: 3.382 |  Val. PPL:  29.426\n",
            "Epoch: 10 | Time: 9s\n",
            "\tTrain Loss: 3.140 | Train PPL:  23.113\n",
            "\t Val. Loss: 3.377 |  Val. PPL:  29.283\n",
            "Epoch: 11 | Time: 9s\n",
            "\tTrain Loss: 3.112 | Train PPL:  22.465\n",
            "\t Val. Loss: 3.371 |  Val. PPL:  29.122\n",
            "Epoch: 12 | Time: 9s\n",
            "\tTrain Loss: 3.080 | Train PPL:  21.755\n",
            "\t Val. Loss: 3.352 |  Val. PPL:  28.548\n",
            "Epoch: 13 | Time: 9s\n",
            "\tTrain Loss: 3.052 | Train PPL:  21.158\n",
            "\t Val. Loss: 3.349 |  Val. PPL:  28.488\n",
            "Epoch: 14 | Time: 9s\n",
            "\tTrain Loss: 3.025 | Train PPL:  20.584\n",
            "\t Val. Loss: 3.346 |  Val. PPL:  28.377\n",
            "Epoch: 15 | Time: 9s\n",
            "\tTrain Loss: 2.997 | Train PPL:  20.031\n",
            "\t Val. Loss: 3.332 |  Val. PPL:  28.000\n",
            "Epoch: 16 | Time: 9s\n",
            "\tTrain Loss: 2.972 | Train PPL:  19.528\n",
            "\t Val. Loss: 3.333 |  Val. PPL:  28.032\n",
            "Epoch: 17 | Time: 9s\n",
            "\tTrain Loss: 2.947 | Train PPL:  19.054\n",
            "\t Val. Loss: 3.332 |  Val. PPL:  27.995\n",
            "Epoch: 18 | Time: 9s\n",
            "\tTrain Loss: 2.926 | Train PPL:  18.655\n",
            "\t Val. Loss: 3.325 |  Val. PPL:  27.803\n",
            "Epoch: 19 | Time: 9s\n",
            "\tTrain Loss: 2.910 | Train PPL:  18.348\n",
            "\t Val. Loss: 3.326 |  Val. PPL:  27.834\n",
            "Epoch: 20 | Time: 9s\n",
            "\tTrain Loss: 2.901 | Train PPL:  18.184\n",
            "\t Val. Loss: 3.324 |  Val. PPL:  27.774\n",
            "\n",
            "Finished Training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "\n",
        "    # Tokenize the source sentence if it's not already a list\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = [token.lower() for token in tokenize_de(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # Add <sos> and <eos> tokens\n",
        "    tokens = [SRC_VOCAB.lookup_token(SOS_IDX)] + tokens + [SRC_VOCAB.lookup_token(EOS_IDX)]\n",
        "\n",
        "    # Convert tokens to numerical indices\n",
        "    src_indexes = [SRC_VOCAB[token] for token in tokens]\n",
        "\n",
        "    # Convert to a tensor and add a batch dimension\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    # Create the source mask\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    # Pass the source through the encoder\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    # Start the decoder output with the <sos> token\n",
        "    trg_indexes = [TRG_VOCAB[special_symbols[SOS_IDX]]]\n",
        "\n",
        "    # Loop to generate the translation word by word\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # Get the predicted next token (the last one in the sequence)\n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        # If the model predicts the <eos> token, stop\n",
        "        if pred_token == TRG_VOCAB[special_symbols[EOS_IDX]]:\n",
        "            break\n",
        "\n",
        "    # Convert the output indices back to tokens\n",
        "    trg_tokens = [TRG_VOCAB.lookup_token(i) for i in trg_indexes]\n",
        "\n",
        "    # Return the translation (without the <sos> token)\n",
        "    return trg_tokens[1:], attention\n",
        "\n",
        "# --- Let's try it out! ---\n",
        "\n",
        "# 1. Load the weights from your best model\n",
        "# Make sure you've saved a model from your best epoch\n",
        "net.load_state_dict(torch.load('transformer-model.pt'))\n",
        "\n",
        "# 2. Get an example from the test set using the new dataset object\n",
        "example_idx = 234\n",
        "sample = multi30k_dataset['test'][example_idx]\n",
        "src_text = sample['de']\n",
        "trg_text = sample['en']\n",
        "\n",
        "# Tokenize the source text for printing and for the function\n",
        "src_tokens = tokenize_de(src_text)\n",
        "\n",
        "print(f'Source Sentence: {\" \".join(src_tokens)}')\n",
        "print(f'Target Sentence: {trg_text}')\n",
        "\n",
        "# 3. Translate the sentence\n",
        "# The function expects the sentence as a list of tokens\n",
        "translation, attention = translate_sentence(src_tokens, SRC_VOCAB, TRG_VOCAB, net, device)\n",
        "\n",
        "print(f'Predicted Translation: {\" \".join(translation)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InxcUrgDh50E",
        "outputId": "b3c598b0-1b27-4976-882e-f00f67bff101"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Sentence: Zwei braune Hunde spielen grob miteinander .\n",
            "Target Sentence: Two brown dogs playing in a rough manner.\n",
            "Predicted Translation: Two brown are playing in a wooden floor . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "introducing beam translation(picking best result out of top k translations)"
      ],
      "metadata": {
        "id": "z3ECCW2qxVtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_translate(sentence, src_vocab, trg_vocab, model, device, beam_width=3, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = [token.lower() for token in tokenize_de(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [SRC_VOCAB.lookup_token(SOS_IDX)] + tokens + [SRC_VOCAB.lookup_token(EOS_IDX)]\n",
        "    src_indexes = [SRC_VOCAB[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    # Start with a single beam: ([<sos>], 0 score)\n",
        "    beams = [([TRG_VOCAB[special_symbols[SOS_IDX]]], 0.0)]\n",
        "    completed_beams = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        new_beams = []\n",
        "        for seq, score in beams:\n",
        "            # If a beam has ended, add it to completed_beams and skip it\n",
        "            if seq[-1] == TRG_VOCAB[special_symbols[EOS_IDX]]:\n",
        "                completed_beams.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            trg_tensor = torch.LongTensor(seq).unsqueeze(0).to(device)\n",
        "            trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "\n",
        "            # Get log probabilities for the next word\n",
        "            pred_log_probs = F.log_softmax(output[:,-1], dim=-1)\n",
        "\n",
        "            # Get the top 'k' next words and their log probabilities\n",
        "            top_k_log_probs, top_k_indexes = torch.topk(pred_log_probs, beam_width)\n",
        "\n",
        "            # Create new beams from the top k options\n",
        "            for i in range(beam_width):\n",
        "                new_seq = seq + [top_k_indexes[0][i].item()]\n",
        "                new_score = score + top_k_log_probs[0][i].item()\n",
        "                new_beams.append((new_seq, new_score))\n",
        "\n",
        "        # If all beams have finished, we can stop early\n",
        "        if not new_beams:\n",
        "            break\n",
        "\n",
        "        # Add any completed beams from this step\n",
        "        completed_beams.extend([beam for beam in new_beams if beam[0][-1] == TRG_VOCAB[special_symbols[EOS_IDX]]])\n",
        "\n",
        "        # Prune the new beams: keep only the top 'k' overall\n",
        "        # Exclude beams that have just completed\n",
        "        uncompleted_new_beams = [beam for beam in new_beams if beam[0][-1] != TRG_VOCAB[special_symbols[EOS_IDX]]]\n",
        "        beams = sorted(uncompleted_new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "        # If we have enough completed beams, we can stop\n",
        "        if len(completed_beams) >= beam_width:\n",
        "            break\n",
        "\n",
        "    # If no beams completed, use the current best ones\n",
        "    if not completed_beams:\n",
        "        completed_beams = beams\n",
        "\n",
        "    # Normalize scores by length and find the best one\n",
        "    completed_beams.sort(key=lambda x: x[1]/len(x[0]), reverse=True)\n",
        "    best_seq = completed_beams[0][0]\n",
        "\n",
        "    trg_tokens = [TRG_VOCAB.lookup_token(i) for i in best_seq]\n",
        "\n",
        "    return trg_tokens[1:], None # Attention is not returned in this simplified version"
      ],
      "metadata": {
        "id": "mergG25UxVEJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the weights from your best model\n",
        "net.load_state_dict(torch.load('transformer-model.pt'))\n",
        "\n",
        "# Get an example from the test set\n",
        "example_idx = 128\n",
        "sample = multi30k_dataset['test'][example_idx]\n",
        "src_text = sample['de']\n",
        "trg_text = sample['en']\n",
        "src_tokens = tokenize_de(src_text)\n",
        "\n",
        "print(f'Source Sentence: {\" \".join(src_tokens)}')\n",
        "print(f'Target Sentence: {trg_text}\\n')\n",
        "\n",
        "# --- Greedy Search (beam_width = 1) ---\n",
        "greedy_translation, _ = beam_search_translate(src_tokens, SRC_VOCAB, TRG_VOCAB, net, device, beam_width=1)\n",
        "print(f'Greedy Translation: {\" \".join(greedy_translation)}')\n",
        "\n",
        "# --- Beam Search (beam_width = 3) ---\n",
        "beam_translation, _ = beam_search_translate(src_tokens, SRC_VOCAB, TRG_VOCAB, net, device, beam_width=3)\n",
        "print(f'Beam Search (k=3) Translation: {\" \".join(beam_translation)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbIzGt5mxeG6",
        "outputId": "062d435d-2a7b-41b1-f61a-c87f360fda72"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Sentence: Eine alte Frau sitzt an einem Webstuhl und stellt Stoff her .\n",
            "Target Sentence: An old woman working at a loom making cloth.\n",
            "\n",
            "Greedy Translation: An old worker is sitting on a metal ramp and an outdoor area . <eos>\n",
            "Beam Search (k=3) Translation: An old worker is sitting on a ramp and an urban area <eos>\n"
          ]
        }
      ]
    }
  ]
}